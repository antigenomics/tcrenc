{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00301fa1-f9c9-44ba-8628-72b3b03862a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cde945d-2e4c-49d6-914a-0594b537d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3378bc-30b7-4dac-a0eb-9a98df0d160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 10\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "capacity = 64\n",
    "learning_rate = 1e-3\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea70a9d-81ba-4364-a699-09b83cb7cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = MNIST(root='./data/MNIST', download=True, train=False, transform=img_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a188f037-2737-4ea3-9ef4-0bf2d3b017c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 396171\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        c = capacity\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1) # out: c x 14 x 14\n",
    "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1) # out: c x 7 x 7\n",
    "        self.fc = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        c = capacity\n",
    "        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*7*7)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), capacity*2, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.tanh(self.conv1(x)) # last layer before output is tanh, since the images are normalized and 0-centered\n",
    "        return x\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        x_recon = self.decoder(latent)\n",
    "        return x_recon\n",
    "    \n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder = autoencoder.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fbdcee-82a1-4468-9d0e-4fa2998c25f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch [1 / 50] average reconstruction error: 0.136919\n",
      "Epoch [2 / 50] average reconstruction error: 0.074327\n",
      "Epoch [3 / 50] average reconstruction error: 0.066702\n",
      "Epoch [4 / 50] average reconstruction error: 0.062492\n",
      "Epoch [5 / 50] average reconstruction error: 0.059851\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=autoencoder.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# set to training mode\n",
    "autoencoder.train()\n",
    "\n",
    "train_loss_avg = []\n",
    "\n",
    "print('Training ...')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_avg.append(0)\n",
    "    num_batches = 0\n",
    "    \n",
    "    for image_batch, _ in train_dataloader:\n",
    "        \n",
    "        image_batch = image_batch.to(device)\n",
    "        \n",
    "        # autoencoder reconstruction\n",
    "        image_batch_recon = autoencoder(image_batch)\n",
    "        \n",
    "        # reconstruction error\n",
    "        loss = F.mse_loss(image_batch_recon, image_batch)\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # one step of the optmizer (using the gradients from backpropagation)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_avg[-1] += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    train_loss_avg[-1] /= num_batches\n",
    "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc1f17-32fc-43ac-ba05-48c326eb369f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcreppred_sil",
   "language": "python",
   "name": "tcreppred_sil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
